{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "COmbine all of the feature calculations into a single notebook and output a full featue set as a dataframe.\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from utils import aws # used to create aws session and load parquet \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast \n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LOB Data\n",
    "\n",
    "Currently this is run using the sample dataset, but going forward will need to be run using the full LOB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sample lob from s3 to a dask dataframe\n",
    "samp_lob_ddf = aws.load_s3_file_as_ddf(\"s3://dsmp-ol2/processed-data/lob_sample_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the dask datafram to a pandas dataframe\n",
    "df = samp_lob_ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string to lists\n",
    "df['Bid'] = df['Bid'].apply(ast.literal_eval)\n",
    "df['Ask'] = df['Ask'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing rows in mid price\n",
    "df = df.dropna(subset=['Mid_Price']) # This assumes the number of missing mid prices is small\n",
    "# TODO following EDA determine if this is the correct decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "## Simple Financial Features\n",
    "\n",
    "### Volume and OBV\n",
    "\n",
    "- OBV -on balance volume- confirm trends by analysing volume changes. Persistnet rising obv suggests accumulation / buying pressure- associated with upward price movement. Persistent falling obv suggests distribution /selling pressure. Theory is that changes in volume preced price movements. Increase in volume often precedes a change in price direction. Use as trend,reversal and breakout confirmation.\n",
    "\n",
    "\n",
    "- If today's close is higher than yesterday's close, then OBV = Previous OBV + today's volume.\n",
    "- If today's close is lower than yesterday's close, then OBV = Previous OBV - today's volume.\n",
    "- If today's close is equal to yesterday's close, then OBV = Previous OBV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate order volume at each timestep\n",
    "df['Volume'] = df['Bid'].apply(lambda x: sum([qty for price, qty in x])) + \\\n",
    "               df['Ask'].apply(lambda x: sum([qty for price, qty in x]))\n",
    "\n",
    "#calc obv\n",
    "#should be done using close but we can approximate with mid but might not be as reliable\n",
    "direction = df['Mid_Price'].diff().apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "df['OBV'] = (direction * df['Volume']).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Book Imbalance -- IGNORE as Using Volume Imbalance\n",
    "\n",
    "Order book imbalance reflects the proportion of buy to sell orders and can indicate potential price movements based on supply and demand dynamics.\n",
    "\n",
    "calc as:\n",
    "\n",
    "$$\n",
    "\\text{Imbalance} = \\frac{Q_{\\text{bid}} - Q_{\\text{ask}}}{Q_{\\text{bid}} + Q_{\\text{ask}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $Q_{\\text{bid}}$ is the total quantity of buy orders,\n",
    "- $Q_{\\text{ask}}$ is the total quantity of sell orders.\n",
    "\n",
    "Positive imbalance suggests a predominance of buy orders, which could indicate upward pressure on prices, while a negative imbalance suggests the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #This is the same as Volume Imbalance. Volume imbalance is clearer.\n",
    "# #calc total quantity\n",
    "# def total_quantity(price_qty_list):\n",
    "#     if not price_qty_list:\n",
    "#         return 0\n",
    "#     total_qty = sum(qty for _, qty in price_qty_list)\n",
    "#     return total_qty\n",
    "\n",
    "# #calc total bid ask quantities\n",
    "# df['Total_Bid_Qty'] = df['Bid'].apply(total_quantity)\n",
    "# df['Total_Ask_Qty'] = df['Ask'].apply(total_quantity)\n",
    "\n",
    "# #calc order book imbalance\n",
    "# df['Order_Book_Imbalance'] = (df['Total_Bid_Qty'] - df['Total_Ask_Qty']) / (df['Total_Bid_Qty'] + df['Total_Ask_Qty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Imbalance\n",
    "\n",
    "Volume Imbalance is a metric used to assess the balance between supply and demand in a market at any given time, based on order book data. It compares the total volume of buy orders (bids) to the total volume of sell orders (asks), providing insight into potential price movements.\n",
    "\n",
    "**Formula**\n",
    "\n",
    "The Volume Imbalance ($VI$) is calculated using the formula:\n",
    "\n",
    "$$ VI = \\frac{V_{\\text{bids}} - V_{\\text{asks}}}{V_{\\text{bids}} + V_{\\text{asks}}} $$\n",
    "\n",
    "where:\n",
    "- $V_{\\text{bids}}$ is the total volume of all bid orders.\n",
    "- $V_{\\text{asks}}$ is the total volume of all ask orders.\n",
    "\n",
    "**Interpretation**\n",
    "\n",
    "- $VI$ ranges from -1 to 1.\n",
    "- A $VI$ closer to 1 indicates a higher volume of bids relative to asks, suggesting upward pressure on price.\n",
    "- A $VI$ closer to -1 indicates a higher volume of asks relative to bids, suggesting downward pressure on price.\n",
    "- A $VI$ around 0 indicates a balance between bid and ask volumes, suggesting a stable market condition without clear directional pressure on price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_volume_imbalance(row):\n",
    "    #calculate total bid volume\n",
    "    total_bid_volume = sum([volume for _, volume in row['Bid']])\n",
    "    \n",
    "    #calculate total ask volume\n",
    "    total_ask_volume = sum([volume for _, volume in row['Ask']])\n",
    "    \n",
    "    #error handling (div by 0)\n",
    "    total_volume = total_bid_volume + total_ask_volume\n",
    "    if total_volume == 0:\n",
    "        return 0  #return 0 imbalance when there are no bids or asks\n",
    "    \n",
    "    #calculate volume imbalance\n",
    "    volume_imbalance = (total_bid_volume - total_ask_volume) / total_volume\n",
    "    return volume_imbalance\n",
    "\n",
    "#apply\n",
    "df['Total_Volume_Imbalance'] = df.apply(calculate_volume_imbalance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Mid-Price by Order Imbalance -- Check this calculation\n",
    "\n",
    "Given **Mid Price** ($M$) is already calculated, we calculate **Order Imbalance** ($I$) and the **Weighted Mid Price** ($M_W$) as follows:\n",
    "\n",
    "**Order Imbalance**\n",
    "With $V_B$ representing the volume of the best bid and $V_A$ representing the volume of the best ask, the order imbalance ($I$) is calculated by the formula:\n",
    "\n",
    "$$ I = \\frac{V_B - V_A}{V_B + V_A} $$\n",
    "\n",
    "**Weighted Mid Price by Order Imbalance**\n",
    "We adjust the mid price based on the order imbalance using the formula:\n",
    "\n",
    "$$ M_W = M \\times (1 + k \\times I) $$\n",
    "\n",
    "Here, $k$ is a scaling factor that modulates the effect of the order imbalance on the mid price. This factor can be empirically determined to best reflect the impact of order volume differences on the price.\n",
    "\n",
    "This approach allows us to adjust the given mid price considering the balance or imbalance between buy and sell orders, reflecting a more nuanced market value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Ask Sam, are Orderbook Imbalance, Order Imbalance and Volume Imbalance the same thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #first we need to empirically define k\n",
    "\n",
    "# #future mid-price \n",
    "# df['Mid_Price_Future'] = df['Mid_Price'].shift(-1) #TODO Check if this is correct? we will not be able to do this on our test set as we can't look into the future\n",
    "\n",
    "# #calc price change\n",
    "# df['Price_Change'] = df['Mid_Price_Future'] - df['Mid_Price']\n",
    "\n",
    "# # TODO check this is correct. It looks like this is calculating the overall volume not best volume?\n",
    "# def calculate_order_imbalance(row):\n",
    "#     if row['Bid'] and row['Ask']:\n",
    "#         volume_bid = sum([bid[1] for bid in row['Bid']])\n",
    "#         volume_ask = sum([ask[1] for ask in row['Ask']])\n",
    "#         return (volume_bid - volume_ask) / (volume_bid + volume_ask)\n",
    "#     return np.nan\n",
    "\n",
    "# #calc order imbalance for each row\n",
    "# df['Order_Imbalance'] = df.apply(calculate_order_imbalance, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use regression to get value for k\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# #filter out nan\n",
    "# filtered_df = df.dropna(subset=['Order_Imbalance', 'Price_Change'])\n",
    "\n",
    "# X = filtered_df[['Order_Imbalance']].values.reshape(-1, 1)\n",
    "# y = filtered_df['Price_Change'].values\n",
    "\n",
    "# #fit linear regression model\n",
    "# model = LinearRegression()\n",
    "# model.fit(X, y)\n",
    "\n",
    "# #coefficient of 'Order_Imbalance' as 'k'\n",
    "# k = model.coef_[0]\n",
    "\n",
    "# print(f\"Derived k value: {k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calc weighted mid price col\n",
    "# import pandas as pd\n",
    "\n",
    "# def calculate_weighted_mid_price(row, k):\n",
    "#     #check if nan\n",
    "#     if pd.isna(row['Mid_Price']):\n",
    "#         return row['Mid_Price']  #return original if nan\n",
    "    \n",
    "#     imbalance = row['Order_Imbalance']\n",
    "    \n",
    "#     #adjust mid price based on imbalance\n",
    "#     weighted_mid_price = row['Mid_Price'] * (1 + k * imbalance)\n",
    "    \n",
    "#     return weighted_mid_price\n",
    "\n",
    "# #apply\n",
    "# df['Weighted_Mid_Price'] = df.apply(calculate_weighted_mid_price, k=k, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bid-Ask Spread and Level-1 Data\n",
    "\n",
    "Bid-ask spread represents the difference between the highest price buyers are willing to pay (**bid price**) and the lowest price sellers are willing to accept (**ask price**). We can use it as a key indicator of market liquidity and efficiency.\n",
    "\n",
    "We calculate it by:\n",
    "\n",
    "$$ \\text{Bid-Ask Spread} = \\text{Ask Price} - \\text{Bid Price} $$\n",
    "\n",
    "where:\n",
    "- $\\text{Ask Price}$ is the lowest price a seller is willing to accept.\n",
    "- $\\text{Bid Price}$ is the highest price a buyer is willing to pay.\n",
    "\n",
    "\n",
    "A narrower (lower) spread often indicates a more liquid market, where transactions can occur more easily at prices close to the market's consensus value. A wider (higher) spread suggests lower liquidity, higher transaction costs, and potentially more volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_level_1(row):\n",
    "    if not row['Bid'] or not row['Ask']:  # check if either list is empty\n",
    "        return pd.Series([None, None, None, None, None, None], index=['Bid_Ask_Spread', 'Level_1_Bid_Price', 'Level_1_Ask_Price', 'Level_1_Bid_Quantity', 'Level_1_Ask_Quantity', 'Level_1_Order_Imbalance'])\n",
    "    \n",
    "    # extract highest bid price and lowest ask price\n",
    "    highest_bid_price = row['Bid'][0][0]  # assuming first entry is the highest bid\n",
    "    lowest_ask_price = row['Ask'][0][0]  # assuming first entry is the lowest ask\n",
    "    highest_bid_qnt = row['Bid'][0][1]  # assuming first entry is the highest bid\n",
    "    lowest_ask_qnt = row['Ask'][0][1]  # assuming first entry is the lowest ask\n",
    "    \n",
    "    # calculate the spread\n",
    "    bid_ask_spread = lowest_ask_price - highest_bid_price\n",
    "\n",
    "    # calculate the level 1 order imbalance\n",
    "    l1_order_imbalance = highest_bid_qnt - lowest_ask_qnt\n",
    "\n",
    "    return pd.Series([bid_ask_spread, highest_bid_price, lowest_ask_price, highest_bid_qnt, lowest_ask_qnt, l1_order_imbalance], index=['Bid_Ask_Spread', 'Level_1_Bid_Price', 'Level_1_Ask_Price', 'Level_1_Bid_Quantity', 'Level_1_Ask_Quantity', 'Level_1_Order_Imbalance'])\n",
    "\n",
    "# apply\n",
    "df[['Bid_Ask_Spread', 'Level_1_Bid_Price', 'Level_1_Ask_Price', 'Level_1_Bid_Quantity', 'Level_1_Ask_Quantity', 'Level_1_Order_Imbalance']] = df.apply(calculate_level_1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Financial Features\n",
    "\n",
    "### RSI\n",
    "\n",
    "RSI (relative strength index)-popular- overbought and oversold conditions-calculates with avg gain, avg loss and relative strength. You need to check direction not just value to understand trend. \n",
    "\n",
    "- above 70- overbought\n",
    "- below 30- oversold\n",
    "- 50-70- a strong bullish trend\n",
    "- 30-50 -a moderate bullish trend\n",
    "- 50-30- a strong bearish trend\n",
    "- 70-40- a moderate bearish trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsi\n",
    "delta = df['Mid_Price'].diff()\n",
    "gain = (delta.where(delta > 0, 0)).rolling(window=20).mean()\n",
    "loss = (-delta.where(delta < 0, 0)).rolling(window=20).mean()\n",
    "RS = gain / loss\n",
    "\n",
    "df['RSI'] = 100 - (100 / (1 + RS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic RSI\n",
    "\n",
    "The Stochastic RSI is an indicator used to identify overbought and oversold conditions by measuring the level of the RSI relative to its high and low range over a specific period. It is derived from the RSI but provides more sensitivity and a higher frequency of trading signals by applying the stochastic oscillator formula to RSI values.\n",
    "\n",
    "$$ \\text{StochRSI} = \\frac{\\text{RSI} - \\text{Min(RSI, n)}}{\\text{Max(RSI, n)} - \\text{Min(RSI, n)}} $$\n",
    "\n",
    "where:\n",
    "- $\\text{RSI}$ is the current value of the Relative Strength Index,\n",
    "- $\\text{Min(RSI, n)}$ is the minimum RSI value over the last $n$ periods,\n",
    "- $\\text{Max(RSI, n)}$ is the maximum RSI value over the last $n$ periods,\n",
    "- $n$ is the specified period over which the highs and lows are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min and max RSI values over window\n",
    "min_rsi = df['RSI'].rolling(window=20).min()\n",
    "max_rsi = df['RSI'].rolling(window=20).max()\n",
    "\n",
    "#calc Stochastic RSI\n",
    "df['Stochastic_RSI'] = (df['RSI'] - min_rsi) / (max_rsi - min_rsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale to 0-100 range (better for interpretation and comparison to other oscillators)\n",
    "df['Stochastic_RSI'] = df['Stochastic_RSI'] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awesome Oscillator\n",
    "\n",
    "The awesome oscillator is a market momentum indicator that compares the recent market momentum, with the momentum over a broader timeframe, by calculating the difference between a 34 period and a 5 period simple moving averages (SMA) of the median prices (or mid prices when median prices are not available).\n",
    "Calc by-\n",
    "\n",
    "$$\n",
    "AO = SMA_{5} - SMA_{34}\n",
    "$$\n",
    "\n",
    "where\n",
    "- $SMA_{5}$ is the 5-period simple moving average of the mid-prices.\n",
    "- $SMA_{34}$ is the 34-period simple moving average of the mid-prices.\n",
    "\n",
    "The AO serves to detect changes in the market's momentum and potentially signal upcoming market reversals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate sma for 34 and 5 periods\n",
    "sma_34 = df['Mid_Price'].rolling(window=34, min_periods=1).mean()\n",
    "sma_5 = df['Mid_Price'].rolling(window=5, min_periods=1).mean()\n",
    "\n",
    "#calc ao\n",
    "df['Awesome_Oscillator'] = sma_5 - sma_34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator Oscillator\n",
    "\n",
    "The accelerator oscillator measures the acceleration or deceleration of the current market driving force, essentially indicating whether the market force is increasing or decreasing.\n",
    "\n",
    "Its calculated as the difference between the 5-period simple moving average of the awesome oscillator, and the ao itself:\n",
    "\n",
    "$$\n",
    "AC = AO - SMA_{5}(AO)\n",
    "$$\n",
    "\n",
    "where-\n",
    "- $AO$ is the awesome oscillator.\n",
    "- $SMA_{5}(AO)$ is the 5-period simple moving average of the awesome oscillator values.\n",
    "\n",
    "It can be used to confirm ao signals or predict possible reversals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_sma_5 = df['Awesome_Oscillator'].rolling(window=5, min_periods=1).mean()\n",
    "df['Accelerator_Oscillator'] = df['Awesome_Oscillator'] - ao_sma_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average Convergence/Divergence Oscillator\n",
    "\n",
    "MACD is a trend-following momentum indicator that shows the relationship between two moving averages of a security’s price.\n",
    "\n",
    "$$\n",
    "\\text{MACD} = EMA_{12}(\\text{price}) - EMA_{26}(\\text{price})\n",
    "$$\n",
    "$$\n",
    "\\text{Signal Line} = EMA_{9}(\\text{MACD})\n",
    "$$\n",
    "\n",
    "where\n",
    "- $EMA_{12}$ and $EMA_{26}$ are the exponential moving averages for 12 and 26 periods, respectively.\n",
    "- The Signal Line is the exponential moving average of the MACD itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc macdd and signal\n",
    "\n",
    "#12 and 26 industry standards\n",
    "ema_12 = df['Mid_Price'].ewm(span=12, adjust=False).mean()\n",
    "ema_26 = df['Mid_Price'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "df['MACD'] = ema_12 - ema_26\n",
    "df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hull Moving Average\n",
    "\n",
    "Hull ma reduces the lag of traditional moving averages with improved smoothing and responsiveness.\n",
    "\n",
    "Calc a weighted moving average with period half the length of the Hull MA, then calculate a WMA for the full period of the Hull MA and subtract it from the first WMA calculation, and finally, calculate a WMA of the result with a period the square root of the Hull MA length.\n",
    "\n",
    "$$\n",
    "\\text{Hull MA} = WMA(2 * \\text{WMA}(n/2) - \\text{WMA}(n), \\sqrt{n})\n",
    "$$\n",
    "\n",
    "where\n",
    "- $WMA(n)$ is the weighted moving average over $n$ periods.\n",
    "- $\\sqrt{n}$ is the square root of the period $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc\n",
    "import numpy as np\n",
    "\n",
    "hull_ma_period = int(np.sqrt(9)) #9 isn't industry standard just suggested starting point- may want to experiment here\n",
    "\n",
    "wma_9 = df['Mid_Price'].rolling(window=9).apply(lambda x: np.dot(x, np.arange(1, 10)) / np.sum(\n",
    "    np.arange(1, 10)), raw=True)\n",
    "\n",
    "df['Hull_MA'] = wma_9.rolling(window=hull_ma_period).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keltner Channels\n",
    "\n",
    "Keltner channels are volatility based envelopes set above and below an exponential moving average.\n",
    "\n",
    "$$\n",
    "\\text{Middle Line} = EMA_{20}(\\text{price})\n",
    "$$\n",
    "$$\n",
    "\\text{Upper Channel Line} = \\text{Middle Line} + 2 \\times ATR_{20}\n",
    "$$\n",
    "$$\n",
    "\\text{Lower Channel Line} = \\text{Middle Line} - 2 \\times ATR_{20}\n",
    "$$\n",
    "\n",
    "where\n",
    "- $EMA_{20}$ is the 20-period exponential moving average.\n",
    "- $ATR_{20}$ is the 20-period average true range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc\n",
    "ema_20 = df['Mid_Price'].ewm(span=20, adjust=False).mean()\n",
    "\n",
    "atr = df['Mid_Price'].rolling(window=20).apply(lambda x: np.max(x) - np.min(x), raw=True)\n",
    "\n",
    "df['Keltner_Channel_Middle'] = ema_20\n",
    "df['Keltner_Channel_Upper'] = ema_20 + 2 * atr\n",
    "df['Keltner_Channel_Lower'] = ema_20 - 2 * atr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detrended Price Oscillator\n",
    "\n",
    "Detrended price oscillator is an indicator designed to remove the trend from price and allow the measurement of the length and magnitude of price cycles from peak to peak or trough to trough.\n",
    "\n",
    "DPO is calculated by subtracting the displaced moving average from the price \\( \\frac{lookback\\ period}{2} + 1 \\) periods ago.\n",
    "\n",
    "$$\n",
    "\\text{DPO} = P_{t - \\left(\\frac{\\text{lookback period}}{2} + 1\\right)} - SMA_{t - \\left(\\frac{\\text{lookback period}}{2}\\right)}\n",
    "$$\n",
    "\n",
    "where\n",
    "- \\( P_{t} \\) is the price at time \\( t \\).\n",
    "- \\( SMA \\) is the simple moving average over the lookback period.\n",
    "- The lookback period is the number of periods used to calculate the SMA and displace it.\n",
    "\n",
    "By removing trends from the price data, the DPO helps to identify cycles and overbought or oversold conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#efine lookback period\n",
    "lookback_period = 20\n",
    "\n",
    "#calc sma\n",
    "df['SMA_20'] = df['Mid_Price'].rolling(window=lookback_period).mean() #may already have this from noise suppression?\n",
    "\n",
    "#calc detrended \n",
    "#shift sma backwards by (lookback_period / 2 + 1) periods\n",
    "df['DPO'] = df['Mid_Price'].shift(int(lookback_period / 2 + 1)) - df['SMA_20'].shift(int(lookback_period / 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bollinger Bands\n",
    "\n",
    "- Bollinger Bands- moveing average calc over specified period e.g. 20 days. + bands for moving average + or - 2 Standard deviations. (Could be 1.5 or 2.5) 1.9-2.2 is usual for algorithmic trading. Identify volatility and potential trend reversals. When prices are near the upper band the asset may be overbought indicating a possible reversal or pullback. When prices are near the lower band- asset may be oversold suggesting a upward reversal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bollinger bands\n",
    "MA = df['Mid_Price'].rolling(window=20).mean() #over 20 periods\n",
    "SD = df['Mid_Price'].rolling(window=20).std()\n",
    "\n",
    "df['Upper_BB'] = MA + (2 * SD) #upper bb\n",
    "df['Lower_BB'] = MA - (2 * SD) #lower bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"darkgrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(df.index, df['Mid_Price'], label='Price', color='blue')\n",
    "plt.plot(df.index, df['Upper_BB'], label='Upper Bollinger Band', linestyle='--', color='red')\n",
    "plt.plot(df.index, df['Lower_BB'], label='Lower Bollinger Band', linestyle='--', color='green')\n",
    "\n",
    "#truncate to see pattern\n",
    "start_index = 16500  \n",
    "end_index = 17500  \n",
    "plt.xlim(start_index, end_index)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid_Price')\n",
    "plt.title('Mid price and bollinger bands')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realised Semi Variance\n",
    "\n",
    "Realised semi-variance is a measure of the downside risk of an asset's returns. It's similar to realised variance but considers only the returns that fall below a certain threshold (typically, returns less than zero, focusing on negative returns). Realised semi-variance provides insight into the volatility resulting from negative market movements, which is useful for risk management purposes.\n",
    "\n",
    "Calculated by-\n",
    "\n",
    "$$\n",
    "\\text{Realised Semi-Variance} = \\sum_{i=1}^{n} \\min(r_i, 0)^2\n",
    "$$\n",
    "\n",
    "where $r_i$ represents the log return at interval $i$, and $n$ is the total number of intervals.\n",
    "\n",
    "I will need to create this over a rolling window so it can apture how downside volatility changing over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc log returns (natural log of ratio of its final price to its initial price)\n",
    "df['Log_Returns'] = np.log(df['Mid_Price'] / df['Mid_Price'].shift(1))\n",
    "\n",
    "#define window size \n",
    "window_size = 20 #choosing 20 to be consistent with bollinger bands \n",
    "\n",
    "#calculate realised semi-variance for each window\n",
    "df['Realised_Semi_Variance'] = df['Log_Returns'].rolling(window=window_size, min_periods=1).apply(lambda x: np.sum(\n",
    "    np.minimum(x, 0)**2), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count = df['Log_Returns'].isna().sum()\n",
    "\n",
    "print(f\"Number of NaN values in 'Log_Returns': {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Log_Returns'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realised Volatility\n",
    "\n",
    "Realised volatility measures the variation in prices over a period and is a common measure of financial market volatility. For a rolling window calculation, we use log returns and square them to represent variance, which underpins volatility. The realised volatility over the rolling window is the square root of the rolling sum of squared log returns\n",
    "\n",
    "This calculation provides a dynamic measure of volatility that updates over time, capturing the changing risk profile of the asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def rolling window\n",
    "window_size = 20\n",
    "\n",
    "#calc squared log returns\n",
    "df['Squared_Log_Returns'] = df['Log_Returns'] ** 2\n",
    "\n",
    "#calc rolling realised volatility\n",
    "df['Realised_Volatility'] = np.sqrt(df['Squared_Log_Returns'].rolling(window=window_size).sum())\n",
    "\n",
    "print((df['Squared_Log_Returns'] < 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realised Bipower Variation\n",
    "\n",
    "Realised bipower variation is a measure used to estimate the variance of financial asset returns, focusing specifically on the continuous component of price movements and excluding jumps. It is particularly useful in high-frequency finance for analysing volatility under normal market conditions. Calculated as-\n",
    "\n",
    "$$\n",
    "RBV = \\frac{\\pi}{2} \\cdot \\sum_{i=1}^{n-1} |r_i| \\cdot |r_{i+1}|\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $|r_i|$ is the absolute value of the log return at interval $i$,\n",
    "- $n$ is the total number of log returns in the series.\n",
    "\n",
    "RBV isolates the continuous part of price movements by leveraging the properties of bipower measures, which provides us with a robust estimate of volatility that excludes large, discontinuous jumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc absolute log returns\n",
    "df['Abs_Log_Returns'] = df['Log_Returns'].abs()\n",
    "\n",
    "#calc realised bipower variation\n",
    "#using .shift(-1) to get r_{i+1} for each r_i, multiplying adjacent abs returns\n",
    "df['Realised_Bipower_Variation'] = (np.pi / 2) * (df['Abs_Log_Returns'] * df['Abs_Log_Returns'].shift(\n",
    "    -1)).rolling(window=2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jump Variation\n",
    "\n",
    "Jump variation is a measure used to isolate the discontinuous (jump) components in asset prices, distinguishing them from the continuous components typically captured by other volatility measures. This distinction is particularly relevant in financial markets where sudden price movements can have significant implications.\n",
    "\n",
    "\n",
    "**Total quadratic variation** ($TQV$) encompasses both continuous movements and jumps in the asset's price. It is calculated as the sum of squared log returns over a specific period.\n",
    "\n",
    "$$TQV = \\sum_{i=1}^{n} r_i^2$$\n",
    "   where $r_i$ is the log return at interval $i$.\n",
    "\n",
    "\n",
    "**Realised bipower variation** ($RBV$) estimates the continuous component of price movements, excluding jumps, by considering the product of absolute log returns for adjacent periods.\n",
    "\n",
    "$RBV = \\frac{\\pi}{2} \\cdot \\sum_{i=1}^{n-1} |r_i| \\cdot |r_{i+1}|$$\n",
    "\n",
    "This uses adjacent pairs of absolute log returns to approximate the continuous component of volatility.\n",
    "\n",
    "**Realised bipower variation** ($JV$) isolates the jump component by subtracting the continuous component ($RBV$) from the total variation ($TQV$).\n",
    "\n",
    "$$JV = TQV - RBV$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squared log returns already calculated\n",
    "\n",
    "#calc total quadratic variation as the sum of squared log returns\n",
    "df['Total_Quadratic_Variation'] = df['Squared_Log_Returns'].rolling(window=20).sum()\n",
    "\n",
    "#calc jump variation\n",
    "df['Jump_Variation'] = df['Total_Quadratic_Variation'] - df['Realised_Bipower_Variation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot Volatility\n",
    "\n",
    "Spot volatility reflects the instantaneous volatility of an asset, capturing the expected level of fluctuation in its price at any given moment. This approach is particularly useful in high-frequency trading and market microstructure analysis, where actual transaction data may be sparse but quote data (bids and asks) are available.\n",
    "\n",
    "**GARCH Model**\n",
    " The GARCH model is then fitted to the log returns. GARCH models are used to estimate time-varying volatility, taking into account the conditional variance of past errors and past variances.\n",
    "   \n",
    "A basic GARCH(1,1) model includes one lag of squared residuals and one lag of conditional variance, capturing the clustering of volatility over time.\n",
    "\n",
    "**Forecasting Spot Volatility**\n",
    "Using the fitted GARCH model, forecast next periodvolatility to estimate the spot volatility. The forecasted variance ($\\hat{\\sigma}^2_t$) from the GARCH model provides an estimate of the expected volatility for the next period:\n",
    "\n",
    "   $$ \\text{Spot Volatility} = \\sqrt{\\hat{\\sigma}^2_t} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log returns already calculated\n",
    "\n",
    "from arch import arch_model\n",
    "\n",
    "#fit GARCH(1,1) model\n",
    "am = arch_model(df['Log_Returns']*100, vol='Garch', p=1, q=1, mean='Zero')\n",
    "res = am.fit(update_freq=5, disp='off')\n",
    "             \n",
    "             \n",
    "#forecast next period's volatility \n",
    "forecasts = res.forecast(horizon=1)\n",
    "\n",
    "#extract last forecasted variance and take the square root for spot volatility\n",
    "df['Spot_Volatility'] = np.sqrt(forecasts.variance.iloc[-1] / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsmp-ol2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
